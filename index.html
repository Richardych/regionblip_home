<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Bubo GPT">
  <meta name="keywords" content="multimodal chatbot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>BuboGPT</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./images/logo.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><img id="painting_icon" width="5%" src="./images/logo.png"> BuboGPT:</h1>
            <h3 class="title is-3 publication-title">Enabling Visual Grounding in Multi-Modal LLMs</h3>
            <div class="is-size-5">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=uPmTOHAAAAAJ&hl=en" style="color:#f68946;font-weight:normal;">Yang Zhao<sup>*</sup></a>,
              </span>
              <span class="author-block">
		      <a href="https://scholar.google.com/citations?user=xXMj6_EAAAAJ&hl=zh-CN" style="color:#f68946;font-weight:normal;">Zhijie Lin<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=DdCAbWwAAAAJ&hl=en" style="color:#f68946;font-weight:normal;">Daquan Zhou</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.sg/citations?user=GW9vw8UAAAAJ&hl=en" style="color:#f68946;font-weight:normal;">Zilong Huang</a>,
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/site/jshfeng/" style="color:#f68946;font-weight:normal;">Jiashi Feng</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.sg/citations?user=NmHgX-wAAAAJ&hl=en" style="color:#f68946;font-weight:normal;">Bingyi Kang<sup>+</sup></a>,
              </span>
            </div>

            <br>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> Bytedance Inc.</span>
              <span class="author-block">&nbsp&nbsp<sup>*</sup>Equal Contribution</span>
              <span class="author-block">&nbsp&nbsp<sup>+</sup>Project Lead</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2307.08581" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/magic-research/bubogpt" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/spaces/magicr/BuboGPT" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Demo</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/magicr/BuboGPT/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/magicr/BuboGPT-ckpt/resolve/main/bubogpt_7b.pth" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-share-square"></i>
                    </span>
                    <span>Model</span>
                  </a>
                </span>


                

                <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                  </a>
              </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h4 class="subtitle has-text-centered">
	  BuboGPT is an advanced Large Language Model (LLM) that incorporates multi-modal inputs including text, image and audio, 
	  with a unique ability to ground its responses to visual objects. It demonstrates remarkable chat abilities for arbitrary 
	  image-audio data understanding, whether aligned or unaligned.
        </h4>

        <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <iframe width="900" height="500" src="https://www.youtube.com/embed/uRdmC3wPz6k" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
      <div class="is-size-6 publication-authors">
        <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> Bubo owls are well known for having strong
          <strong>vision</strong> and <strong>hearing</strong> abilities that help them thrive.</span>
      </div>
    </div>
  </div>
      </div>
    </div>
  </section>


  <section class="section"  style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              LLMs have demonstrated remarkable abilities at interacting with humans through language, especially with the usage of instruction-following data. Recent advancements in LLMs, such as MiniGPT-4, LLaVA, and X-LLM, further enlarge their abilities by incorporating multi-modal inputs, including image, video, and speech. 
		    Despite their effectiveness at generating precise and detailed language understanding of the given modality signal, these LLMs give up the ability to ground specific parts of inputs, thus only constructing a coarse-grained mapping. However, explicit and informative correspondence between text and other modalities will not only improve the user experience but also help to expand the application scenario of multi-modal LLMs.
              <ol type="1">
                <li><b>BuboGPT Architecture </b>. <span style="font-size: 95%;">We build a multi-modal LLM, BuboGPT for multi-modal understanding including image, audio and text by learning a common semantic space and further explore the fine-grained relation between different visual objects and different modalities. </span></li>
                <li><b>Multimodal Instruct Data</b>. <span style="font-size: 95%;">We construct a high-quality multi-modal instruction-tuning dataset including fine-grained audio descriptions and cross-modal sound localization, and introduce both positive and negative image-audio pairs for semantic matching to facilitate the cross-modal understanding..</li>
              </ol>  
           </p>
  
          </div>
        </div>
      </div>
        
    </div>
  </section>


  


</body>

</html>
